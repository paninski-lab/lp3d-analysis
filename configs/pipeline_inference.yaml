#TODO 
# ask Matt about adding the views variable to the config file so we can pick the views that we have and in that way instead of iterating over files we will iterate over views - which will determine things

# dataset_name: chickadee-crop
# dataset_name: fly-anipose
dataset_name: mirror-mouse-separate
# dataset_name: rat7m-crop
# dataset_name: human36m-crop

lightning_pose_config: /teamspace/studios/this_studio/lp3d-analysis/configs/config_${dataset_name}.yaml

# [needed?] pipeline seed for initial data split
pipeline_seeds: 0

outputs_dir: /teamspace/studios/this_studio/outputs/${dataset_name}

intermediate_results_dir: test_100_MVT_mirror-mouse-separate_after_merge_debug

# initial training of an ensemble of networks
train_networks:
  # run this section?
  run: True
  # overwrite previous results?
  overwrite: False
  
  # pose estimation data type
  # data_type: lp
  # ensemble seeds
  ensemble_seeds:
    - 0
    - 1
    # - 2
    # - 3
    # - 4
    # - 5
    # - 6
    
  # number of ground truth labels for training
  n_hand_labels: 
    - 100
  # model type
  model_types:
    # - supervised
    # - context
    # - semisupervised
    # - multiview_cnn
    # - multiview_transformer_learnable_crossview
    - multiview_transformer
    # - mvt_3d_loss
    # - mvt_semisupervised
    
    # think about adding multiview here 
  # run inference on videos in these directories (independent of training videos)
  inference_dirs:
    - videos_new
    # - videos_fix
    # - videos_fix_new
    # - videos-for-each-labeled-frame
    # - videos_debug
    # - videos_ind
    # - videos 
    # - videos-for-each-labeled-frame
    # - videos_paper

  # training parameters
  min_steps: 500 # need 5000 I use 500 for testing 
  max_steps: 500 # need 5000
  unfreezing_step: 400 #400 # started with 260 for supervised - should be 400 for transformer 
  milestone_steps: [2000, 3000, 4000] # in general we have [2000, 3000, 4000]
  val_check_interval: 10 # was 5 
  train_check_interval: 5

# This is if we want to run concat or use post_process.py
post_processing_labels:
  eks_singleview:
    run: False
    overwrite: False
  eks_multiview:
    run: False
    overwrite: False
  ensemble_mean:
    run: False
    overwrite: False
  ensemble_median:
    run: False
    overwrite: False 

# post-processing options - coming from the full videos - this is if we extract from the full videos - for the fly 
post_processing_labeled_frames:
  run : False
  overwrite: False
  modes:
    # - eks_singleview
    # - eks_multiview
    # - ensemble_median
    # - ensemble_mean

post_processing_videos:
  eks_multiview:
    run: False
    overwrite: False
    n_latent: 3 # components for pca object - 6 for chickadee and 3 for fly and mouse 
  eks_singleview:
    run: False
    overwrite: False
  ensemble_median:
    run: False
    overwrite: False
  ensemble_mean:
    run: False
    overwrite: False


# visualization options
visualization:
  run: False
  overwrite: False 
  ens_var_plots: False
  pixel_error_ens_std_plots:
    run: False 
    overwrite: False
  eks_singleview_plots:
    run: False
    overwrite: False

  
# remember that we will have to run it over view 
