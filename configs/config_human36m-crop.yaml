data:
  # resize dimensions to streamline model creation
  image_resize_dims:
    height: 256
    width: 256
  # ABSOLUTE path to data directory
  data_dir:  /teamspace/studios/data/human36m-crop
  # ABSOLUTE path to unlabeled videos' directory
  video_dir: /teamspace/studios/data/human36m-crop/videos
  # location of labels; for example script, this should be relative to `data_dir`
  camera_params_file: /teamspace/studios/data/human36m-crop/calibrations.csv
  
  csv_file:
    - CollectedData_ca_01.csv
    - CollectedData_ca_02.csv
    - CollectedData_ca_03.csv
    - CollectedData_ca_04.csv
  bbox_file:
    - bboxes_ca_01.csv
    - bboxes_ca_02.csv
    - bboxes_ca_03.csv
    - bboxes_ca_04.csv
  view_names:
    - ca_01
    - ca_02
    - ca_03
    - ca_04
  
  # downsample heatmaps - 2 | 3
  downsample_factor: 2
  # total number of keypoints
  num_keypoints: 17
  # keypoint names
  keypoint_names:
    - bottom_torso #0
    - l_hip #1
    - l_knee #2
    - l_foot #3
    - r_hip #4
    - r_knee #5
    - r_foot #6
    - center_torso #7
    - upper_torso #8
    - neck_base #9
    - center_head #10
    - r_shoulder #11
    - r_elbow #12
    - r_hand #13
    - l_shoulder #14
    - l_elbow #15
    - l_hand #16
  mirrored_column_matches: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]
  columns_for_singleview_pca: []

  skeleton:
    - ['bottom_torso', 'l_hip']
    - ['bottom_torso', 'r_hip']        # [0, 4]
    # left leg
    - ['l_hip', 'l_knee']              # [1, 2]
    - ['l_knee', 'l_foot']             # [2, 3]
    # right leg
    - ['r_hip', 'r_knee']              # [4, 5]
    - ['r_knee', 'r_foot']             # [5, 6]
    # torso → head
    - ['bottom_torso', 'center_torso'] # [0, 7]
    - ['center_torso', 'upper_torso']  # [7, 8]
    - ['upper_torso', 'neck_base']     # [8, 9]
    - ['neck_base', 'center_head']     # [9, 10]
    # right arm
    - ['upper_torso', 'r_shoulder']    # [8, 11]
    - ['r_shoulder', 'r_elbow']        # [11, 12]
    - ['r_elbow', 'r_hand']            # [12, 13]
    # left arm
    - ['upper_torso', 'l_shoulder']    # [8, 14]
    - ['l_shoulder', 'l_elbow']        # [14, 15]
    - ['l_elbow', 'l_hand']            # [15, 16]
  
  skeleton_colors:
    # pelvis
    bottom_torso-l_hip: pink
    bottom_torso-r_hip: pink
    # left leg
    l_hip-l_knee: dodgerblue
    l_knee-l_foot: dodgerblue
    # right leg
    r_hip-r_knee: darkblue
    r_knee-r_foot: darkblue
    # torso → head
    bottom_torso-center_torso: red
    center_torso-upper_torso: red
    upper_torso-neck_base: red
    neck_base-center_head: red
    # right arm
    upper_torso-r_shoulder: limegreen
    r_shoulder-r_elbow: limegreen
    r_elbow-r_hand: limegreen
    # left arm
    upper_torso-l_shoulder: yellow
    l_shoulder-l_elbow: yellow
    l_elbow-l_hand: yellow

 
  #   'skeleton_colors': [
  #       # pelvis
  #       'pink',
  #       'pink',
  #       # l leg
  #       'dodgerblue',
  #       'dodgerblue',
  #       # r leg
  #       'darkblue',
  #       'darkblue',
  #       # torso-head
  #       'red',
  #       'red',
  #       'red',
  #       'red',
  #       # r arm
  #       'limegreen',
  #       'limegreen',
  #       'limegreen',
  #       # l arm
  #       'yellow',
  #       'yellow',
  #       'yellow',
  #   ],

training:
  # select from one of several predefined image/video augmentation pipelines
  # default- resizing only
  # dlc- imgaug pipeline implemented in DLC 2.0 package
  # dlc-top-down- dlc augmentations plus vertical and horizontal flips
  imgaug: dlc

  imagug_3d: True  # This allows you to use regular dlc augmentations even with camera params
  # batch size of labeled data during training
  train_batch_size: 8
  # batch size of labeled data during validation
  val_batch_size: 48
  # batch size of labeled data during test
  test_batch_size: 48
  # fraction of labeled data used for training
  train_prob: 0.95
  # fraction of labeled data used for validation (remaining used for test)
  val_prob: 0.05
  # <=1 - fraction of total train frames (determined by `train_prob`) used for training
  # >1 - number of total train frames used for training
  train_frames: 1
  # number of gpus to train a single model
  num_gpus: 1
  # number of cpu workers for data loaders
  num_workers: 4
  # epochs over which to assess validation metrics for early stopping
  early_stop_patience: 3
  # epoch at which backbone network weights begin updating
  unfreezing_epoch: 20
  # max training epochs; training may exit before due to early stopping
  min_epochs: 300
  max_epochs: 300
  # frequency to log training metrics (one step is one batch)
  log_every_n_steps: 10
  # frequency to log validation metrics
  check_val_every_n_epoch: 5
  # rng seed for labeled batches
  rng_seed_data_pt: 0
  # rng seed for weight initialization
  rng_seed_model_pt: 0
  optimizer: Adam
  optimizer_params:
    learning_rate: 0.00005 #0.001 #1e-3
  # learning rate scheduler
  # multisteplr | [todo - reducelronplateau]
  lr_scheduler: multisteplr
  lr_scheduler_params:
    multisteplr:
      # milestones: [150, 200, 250]
      milestones: [2000, 3000, 4000]
      gamma: 0.5
  uniform_heatmaps_for_nan_keypoints: true
  
  # curriculum learning configuration for patch masking
  patch_mask:
    init_step: 700
    final_step: 5000
    init_ratio: 0.0
    final_ratio: 0.5

model:
  # list of unsupervised losses
  # "pca_singleview" | "pca_multiview" | "temporal" | "unimodal_mse" | "unimodal_kl"
  losses_to_use: []
  # backbone network:
  # resnet18 | resnet34 | resnet50 | resnet101 | resnet152 | resnet50_contrastive
  # resnet50_animalpose_apose | resnet50_animal_ap10k
  # resnet50_human_jhmdb | resnet50_human_res_rle | resnet50_human_top_res
  # efficientnet_b0 | efficientnet_b1 | efficientnet_b2
  backbone: vits_dino #resnet50_animal_ap10k # vits_dino
  # prediction mode - "heatmap" | "regression" | "heatmap_mhcrnn" | "heatmap_multiview" | "heatmap_multiview_transformer"
  model_type: heatmap_multiview_transformer #heatmap
  # head mode - "heatmap_cnn" | "feature_transformer" | "feature_transformer_learnable" | "feature_transformer_learnable_crossview"
  head: heatmap_cnn
  # which heatmap loss to use
  # "mse" | "kl" | "js"
  heatmap_loss_type: mse
  # tt expt name
  model_name: test_model

dali:
  general:
    seed: 123456
  base:
    train:
      sequence_length: 24
    predict:
      sequence_length: 32
  context:
    train:
      batch_size: 16
    predict:
      sequence_length: 32

losses:
  # loss = projection onto the discarded eigenvectors
  pca_multiview:
    # weight in front of PCA loss
    log_weight: 5.0
    # predictions whould lie within the low-d subspace spanned by these components
    components_to_keep: 3
    # absolute error (in pixels) below which pca loss is zeroed out; if not null, this
    # parameter takes precedence over `empirical_epsilon_percentile`
    epsilon: null
  # loss = projection onto the discarded eigenvectors
  pca_singleview:
    # weight in front of PCA loss
    log_weight: 5.0
    # predictions whould lie within the low-d subspace spanned by components that describe this fraction of variance
    components_to_keep: 0.99
    # absolute error (in pixels) below which pca loss is zeroed out; if not null, this
    # parameter takes precedence over `empirical_epsilon_percentile`
    epsilon: null
  # loss = norm of distance between successive timepoints
  temporal:
    # weight in front of temporal loss
    log_weight: 5.0
    # for epsilon insensitive rectification
    # (in pixels; diffs below this are not penalized)
    epsilon: 20.0
    # nan removal value.
    # (in prob; heatmaps with max prob values are removed)
    prob_threshold: 0.05
  
  supervised_pairwise_projections:
    # weight in front of pairwise projection loss
    log_weight: 0.3
    # absolute error (in pixels) below which pairwise projection loss is zeroed out; if not null, this
    # parameter takes precedence over `empirical_epsilon_percentile`
    epsilon: null

eval:
  # predict? used in scripts/train_hydra.py
  predict_vids_after_training: false
  # str with an absolute path to a directory containing videos for prediction.
  test_videos_directory: ${data.video_dir}
  # save labeled .mp4? used in scripts/train_hydra.py and scripts/predict_new_vids.py
  save_vids_after_training: false
  # matplotlib sequential or diverging colormap name for prediction visualization
  # sequential options: viridis, plasma, magma, inferno, cool, etc.
  # diverging options: RdBu, coolwarm, Spectral, etc.
  colormap: "cool"
  # confidence threshold for plotting a vid
  confidence_thresh_for_vid: 0.90

  # paths to the hydra config files in the output folder, OR absolute paths to such folders.
  # used in scripts/predict_new_vids.py and scripts/create_fiftyone_dataset.py
  hydra_paths: [" "]

  fiftyone:
    # will be the name of the dataset (Mongo DB) created by FiftyOne
    dataset_name: test
    # if you want to manually provide a different model name to be displayed in FiftyOne
    model_display_names: ["test_model"]
    # whether to launch the app from the script (True), or from ipython (and have finer control over the outputs)
    launch_app_from_script: false
    remote: true # for LAI, must be False
    address: 127.0.0.1 # ip to launch the app on.
    port: 5151 # port to launch the app on.

callbacks:
  anneal_weight:
    attr_name: total_unsupervised_importance
    init_val: 0.0
    increase_factor: 0.01
    final_val: 1.0
    freeze_until_epoch: 0

